{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jik19004/anaconda3/envs/Privacy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[0]*5]*5 \n",
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def parser(text:str) -> bool: \n",
    "    tracker = {} # dictionary for the characters\n",
    "    brackets = set(text) # get the brackets \n",
    "    str_array = [element for element in text] # array \n",
    "    \n",
    "    for key in brackets: \n",
    "        bool_indices = [element == key for element in str_array] # the bool indices. \n",
    "        key_list = []\n",
    "        for index in range(len(bool_indices)):\n",
    "            if bool_indices[index]: \n",
    "                key_list.append(index)  \n",
    "        tracker[key] = key_list # set it to our key  \n",
    "    \n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu102\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>BDL_tmpc</th>\n",
       "      <th>BDR_tmpc</th>\n",
       "      <th>DXR_tmpc</th>\n",
       "      <th>GON_tmpc</th>\n",
       "      <th>HFD_tmpc</th>\n",
       "      <th>HVN_tmpc</th>\n",
       "      <th>IJD_tmpc</th>\n",
       "      <th>MMK_tmpc</th>\n",
       "      <th>OXC_tmpc</th>\n",
       "      <th>SNC_tmpc</th>\n",
       "      <th>Demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2011 0:00</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.78</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3053.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/2011 1:00</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.22</td>\n",
       "      <td>3.33</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2892.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/2011 2:00</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>2.78</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.78</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>0.56</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2774.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/2011 3:00</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>3.89</td>\n",
       "      <td>2.22</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/2011 4:00</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>1.67</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2698.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Datetime  BDL_tmpc  BDR_tmpc  DXR_tmpc  GON_tmpc  HFD_tmpc  HVN_tmpc  \\\n",
       "0  1/1/2011 0:00      1.11      1.11      0.00      2.78      1.67      1.11   \n",
       "1  1/1/2011 1:00      1.11      2.22     -0.56      3.33      2.22      3.33   \n",
       "2  1/1/2011 2:00     -0.56      2.78     -1.67      3.33      2.78      2.78   \n",
       "3  1/1/2011 3:00     -1.11      2.22     -1.11      3.89      2.22      1.11   \n",
       "4  1/1/2011 4:00     -1.67      1.67     -1.11      3.33      2.22     -0.56   \n",
       "\n",
       "   IJD_tmpc  MMK_tmpc  OXC_tmpc  SNC_tmpc  Demand  \n",
       "0     -2.78     -1.67       3.0       2.0  3053.0  \n",
       "1     -2.78     -1.67       3.0       3.0  2892.0  \n",
       "2     -2.22      0.56       3.0       3.0  2774.0  \n",
       "3     -2.78      0.56       2.0       4.0  2710.0  \n",
       "4     -2.78     -1.67       4.0       5.0  2698.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "data = pd.read_csv(\"/home/jik19004/FilesToRun/ASOS_10_CT_stations_tmpc_demand_2011_2023.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "WeatherData = pd.read_csv('/home/jik19004/FilesToRun/ASOS_10_CT_stations_tmpc_demand_2011_2023.csv').drop(columns = [\"Unnamed: 0\"])\n",
    "WeatherData.ffill(inplace = True)\n",
    "WeatherData.bfill(inplace = True) # fill in missing values with the previous value.\n",
    "DateTimeCol = WeatherData[\"Datetime\"]\n",
    "HourCol = []\n",
    "WeekDayorWeekEndCol = [] \n",
    "\n",
    "for date in DateTimeCol:\n",
    "    date = datetime.strptime(date, \"%m/%d/%Y %H:%M\")\n",
    "    HourCol.append(date.hour)\n",
    "    if date.weekday() < 5:\n",
    "        WeekDayorWeekEndCol.append(0)\n",
    "    else:\n",
    "        WeekDayorWeekEndCol.append(1)\n",
    "\n",
    "WeatherData.drop(columns = [\"Datetime\"], inplace = True) # drop the datetime column. \n",
    "\n",
    "\n",
    "WeatherData.insert(0, \"Hour\", HourCol)\n",
    "WeatherData.insert(1, \"Weekday or Weekend\", WeekDayorWeekEndCol)\n",
    "DateTimeCol = [datetime.strptime(date, \"%m/%d/%Y %H:%M\") for date in DateTimeCol]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52603\n",
      "70122\n",
      "70123\n",
      "78882\n",
      "78883\n",
      "96403\n"
     ]
    }
   ],
   "source": [
    "train_start = \"1/1/2017 0:00\"\n",
    "train_end = \"12/31/2018 23:00\"\n",
    "\n",
    "val_start = \"1/1/2019 0:00\"\n",
    "val_end = \"12/31/2019 23:00\"\n",
    "\n",
    "test_start = \"1/01/2020 0:00\"\n",
    "test_end = \"12/31/2021 0:00\"\n",
    "\n",
    "\n",
    "date_train_start = datetime.strptime(train_start, \"%m/%d/%Y %H:%M\")\n",
    "date_train_end = datetime.strptime(train_end, \"%m/%d/%Y %H:%M\")\n",
    "\n",
    "date_val_start = datetime.strptime(val_start, \"%m/%d/%Y %H:%M\")\n",
    "date_val_end = datetime.strptime(val_end, \"%m/%d/%Y %H:%M\")\n",
    "\n",
    "date_test_start = datetime.strptime(test_start, \"%m/%d/%Y %H:%M\")\n",
    "date_test_end = datetime.strptime(test_end, \"%m/%d/%Y %H:%M\")\n",
    "\n",
    "DateTimeCol_num = np.array(DateTimeCol)\n",
    "\n",
    "print(np.where(DateTimeCol_num == date_train_start)[0][0])\n",
    "print(np.where(DateTimeCol_num == date_train_end)[0][0])\n",
    "\n",
    "\n",
    "print(np.where(DateTimeCol_num == date_val_start)[0][0])\n",
    "print(np.where(DateTimeCol_num == date_val_end)[0][0])\n",
    "\n",
    "print(np.where(DateTimeCol_num == date_test_start)[0][0])\n",
    "print(np.where(DateTimeCol_num == date_test_end)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index for Jan 1, 2017:  52603\n",
      "index for Dec 31, 2021:  70121\n",
      "index for Jan 1, 2019:  70123\n",
      "index for Dec 31, 2019  78881\n",
      "index for Jan 1, 2019:  78883\n",
      "index for Dec 31, 2019  91289\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(DateTimeCol)):\n",
    "    date = DateTimeCol[i]\n",
    "    if int(date.year == 2017) and int(date.month) == 1 and int(date.day) == 1 and int(date.hour) == 0: # \n",
    "        print(\"index for Jan 1, 2017: \", i)\n",
    "    if int(date.year) == 2018 and int(date.month) == 12 and int(date.day) == 31 and int(date.hour) == 22:\n",
    "        print(\"index for Dec 31, 2021: \", i)\n",
    "    if int(date.year) == 2019 and int(date.month) == 1 and int(date.day) == 1 and int(date.hour) == 0: # \n",
    "        print(\"index for Jan 1, 2019: \", i)\n",
    "    if int(date.year) == 2019 and int(date.month) == 12 and int(date.day) == 31 and int(date.hour) == 22:\n",
    "        print(\"index for Dec 31, 2019 \", i)\n",
    "    if int(date.year) == 2020 and int(date.month) == 1 and int(date.day) == 1 and int(date.hour) == 0: # \n",
    "        print(\"index for Jan 1, 2019: \", i)\n",
    "    if int(date.year) == 2021 and int(date.month) == 5 and int(date.day) == 31 and int(date.hour) == 22: \n",
    "        print(\"index for Dec 31, 2019 \", i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_sequences(data, outputData, input_n_steps, output_n_steps):\n",
    "    X = []\n",
    "    Y = []\n",
    "    length = len(data)\n",
    "    print(length)\n",
    "    for i in range(0,length, 1):\n",
    "        input_indx = i + input_n_steps\n",
    "        #output_indx = input_indx + output_n_steps\n",
    "        output_indx = i + output_n_steps\n",
    "        if (input_indx >= len(data)): # we need to have equally split sequences.\n",
    "            break               # The remaining data that cannot fit into a fixed\n",
    "                                # sequence will immediately be cut!\n",
    "        else:\n",
    "            Xsample = data.iloc[i:input_indx, :] # get the previous data\n",
    "            #Ysample = outputData[input_indx:output_indx]\n",
    "            Ysample = outputData[i]\n",
    "            X.append(Xsample)\n",
    "            Y.append(Ysample)\n",
    "    X = np.asarray(X).astype('float64')\n",
    "    Y = np.asarray(Y).astype('float64')\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "def splitDataAndScale(data, output, train_start = None, train_end = None, val_start = None, val_end = None, test_start = None, test_end = None):\n",
    "    #train_end +=1\n",
    "    #val_end += 1\n",
    "    #test_end += 1\n",
    "    TrainingData = (data.iloc[train_start: train_end + 1, :].copy())\n",
    "    print(len(TrainingData))\n",
    "    TrainingCategories = TrainingData.iloc[:, [0,1]]\n",
    "    TrainingNumerical = TrainingData.iloc[:, 2:]\n",
    "    TrainingOutput = output[train_start + 6: train_end + 2].copy()  \n",
    "    Scaler = StandardScaler().fit(TrainingNumerical)\n",
    "    TrainingNumerical = Scaler.transform(TrainingNumerical)\n",
    "    TrainingCategories = TrainingCategories.reset_index(drop = True)\n",
    "    TrainingData = pd.concat([TrainingCategories, pd.DataFrame(TrainingNumerical)], axis = 1)\n",
    "    TrainingData.reset_index(drop = True, inplace = True)\n",
    "    TrainingOutput.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    ValidationData = data.iloc[val_start: val_end + 1, :].copy()\n",
    "    ValidationData.reset_index(drop = True, inplace = True)\n",
    "    ValidationCategories = ValidationData.iloc[:, [0,1]]\n",
    "    ValidationNumerical = ValidationData.iloc[:, 2:]\n",
    "    ValidationNumerical = Scaler.transform(ValidationNumerical)\n",
    "    ValidationCategories = ValidationCategories.reset_index(drop = True)\n",
    "    ValidationData = pd.concat([ValidationCategories, pd.DataFrame(ValidationNumerical)], axis = 1)\n",
    "    ValidationOutput = output[val_start + 6: val_end + 2].copy()\n",
    "    ValidationData.reset_index(drop = True, inplace = True)\n",
    "    ValidationOutput.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    TestingData = data.iloc[test_start: test_end + 1, :].copy()\n",
    "    TestingData.reset_index(drop = True, inplace = True)\n",
    "    TestingCategories = TestingData.iloc[:, [0,1]]\n",
    "    TestingNumerical = TestingData.iloc[:, 2:]\n",
    "    TestingNumerical = Scaler.transform(TestingNumerical)\n",
    "    TestingCategories = TestingCategories.reset_index(drop = True)\n",
    "    TestingData = pd.concat([TestingCategories, pd.DataFrame(TestingNumerical)], axis = 1)\n",
    "    TestingOutput = output[test_start + 6: test_end + 2].copy()\n",
    "    TestingData.reset_index(drop = True, inplace = True)\n",
    "    TestingOutput.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "    TrainingSequences = return_sequences(TrainingData, TrainingOutput, 6, 1)\n",
    "\n",
    "    TransformedTrainingData = TrainingSequences[0]\n",
    "    TransformedTrainingOutput = TrainingSequences[1]\n",
    "\n",
    "    ValidationSequences = return_sequences(ValidationData, ValidationOutput, 6, 1)\n",
    "\n",
    "    TransformedValidationData = ValidationSequences[0]\n",
    "    TransformedValidationOutput = ValidationSequences[1]\n",
    "\n",
    "    TestingSequences = return_sequences(TestingData, TestingOutput, 6, 1)\n",
    "\n",
    "    TransformedTestingData = TestingSequences[0]\n",
    "    TransformedTestingOutput = TestingSequences[1]\n",
    "\n",
    "\n",
    "    return (TransformedTrainingData, TransformedTrainingOutput, TransformedValidationData, TransformedValidationOutput,\n",
    "    TransformedTestingData, TransformedTestingOutput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17519\n",
      "17519\n",
      "8759\n",
      "12407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "DemandData = WeatherData['Demand'].copy() # The output data\n",
    "WeatherData.drop(columns = ['Demand'], inplace = True)\n",
    "data = splitDataAndScale(WeatherData, DemandData, train_start = 52603, train_end = 70121, val_start = 70123, val_end = 78881, test_start = 78883, test_end = 91289) # splitting the data into training, validation, and testing.\n",
    "\n",
    "\n",
    "TrainingData = data[0]\n",
    "TrainingOutput = data[1]\n",
    "\n",
    "ValidationData = data[2]\n",
    "ValidationOutput = data[3]\n",
    "\n",
    "TestingData = data[4]\n",
    "TestingOutput = data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17513, 6, 12)\n",
      "(8753, 6, 12)\n",
      "(12401, 6, 12)\n"
     ]
    }
   ],
   "source": [
    "print(TrainingData.shape)\n",
    "print(ValidationData.shape)\n",
    "print(TestingData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import PackedSequence\n",
    "from typing import *\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "class VariationalDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies the same dropout mask across the temporal dimension\n",
    "    See https://arxiv.org/abs/1512.05287 for more details.\n",
    "    Note that this is not applied to the recurrent activations in the LSTM like the above paper.\n",
    "    Instead, it is applied to the inputs and outputs of the recurrent layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout: float, batch_first: Optional[bool]=False):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.training or self.dropout <= 0.:\n",
    "            return x\n",
    "\n",
    "        is_packed = isinstance(x, PackedSequence)\n",
    "        if is_packed:\n",
    "            x, batch_sizes = x\n",
    "            max_batch_size = int(batch_sizes[0])\n",
    "        else:\n",
    "            batch_sizes = None\n",
    "            max_batch_size = x.size(0)\n",
    "\n",
    "        # Drop same mask across entire sequence\n",
    "        if self.batch_first:\n",
    "            m = x.new_empty(max_batch_size, 1, x.size(2), requires_grad=False).bernoulli_(1 - self.dropout)\n",
    "        else:\n",
    "            m = x.new_empty(1, max_batch_size, x.size(2), requires_grad=False).bernoulli_(1 - self.dropout)\n",
    "        x = x.masked_fill(m == 0, 0) / (1 - self.dropout)\n",
    "\n",
    "        if is_packed:\n",
    "            return PackedSequence(x, batch_sizes)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class LSTM(nn.LSTM):\n",
    "    def __init__(self, *args, dropouti: float=0.,\n",
    "                 dropoutw: float=0., dropouto: float=0.,\n",
    "                 batch_first=True, unit_forget_bias=True, **kwargs):\n",
    "        super().__init__(*args, **kwargs, batch_first=batch_first)\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "        self.dropoutw = dropoutw\n",
    "        self.input_drop = VariationalDropout(dropouti,\n",
    "                                             batch_first=batch_first)\n",
    "        self.output_drop = VariationalDropout(dropouto,\n",
    "                                              batch_first=batch_first)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Use orthogonal init for recurrent layers, xavier uniform for input layers\n",
    "        Bias is 0 except for forget gate\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight_hh\" in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif \"weight_ih\" in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif \"bias\" in name and self.unit_forget_bias:\n",
    "                nn.init.zeros_(param.data)\n",
    "                param.data[self.hidden_size: 2 * self.hidden_size] = 1\n",
    "\n",
    "    def _drop_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight_hh\" in name:\n",
    "                getattr(self, name).data = \\\n",
    "                    torch.nn.functional.dropout(param.data, p=self.dropoutw,\n",
    "                                                training=self.training).contiguous()\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        self._drop_weights()\n",
    "        input = self.input_drop(input)\n",
    "        seq, state = super().forward(input, hx=hx)\n",
    "        return self.output_drop(seq), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, params1, params2, params3, num_layers, output_size):    \n",
    "    #params1 = [conv_kernel_size, stride, max_kernel_size, LSTM_hidden_size, LSTM_num_layers]\n",
    "    #params2 = [dropout_LSTM, dropout_FFN]\n",
    "    #params3 = [hidden_size1, hidden_size2, hidden_size3]\n",
    "        super(BayesianModel, self).__init__()        \n",
    "        self.conv1D = torch.nn.Conv1d(in_channels = 6, out_channels = 6, kernel_size = params1[0], stride = params1[1])\n",
    "        self.max1D = torch.nn.MaxPool1d(kernel_size = params1[2], stride = params1[1])\n",
    "        self.dropout1 = torch.nn.Dropout(params2[1])\n",
    "        self.input_size = input_size - params1[0] - params1[2] + 2\n",
    "        self.BiLSTM = LSTM(input_size = self.input_size, hidden_size = params1[3], dropout = params2[0], bidirectional = True, num_layers = params1[4])\n",
    "        \n",
    "        layers = []\n",
    "        input_size = params1[3] *2 \n",
    "        num_units = 0 \n",
    "        for i in range(num_layers):\n",
    "            num_units = params3[i] \n",
    "            layers.append(torch.nn.Linear(input_size, num_units, bias = True))\n",
    "            if i!= num_layers-1:\n",
    "                layers.append(torch.nn.BatchNorm1d(num_units)) # add in the batch norm. \n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(params2[1])) #also add in the dropout. \n",
    "            input_size = num_units\n",
    "        \n",
    "        self.intermediate_layers = torch.nn.Sequential(*layers)\n",
    "        self.finalNN = torch.nn.Linear(num_units, output_size, bias = True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1D(x)\n",
    "        x = self.max1D(x) \n",
    "        x = self.dropout1(x) \n",
    "        x = self.BiLSTM(x) \n",
    "        \n",
    "        x = x[0]\n",
    "        x = x[:, -1,:]\n",
    "        \n",
    "        x = self.intermediate_layers(x) \n",
    "        x = self.finalNN(x) \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, output):\n",
    "        data = torch.tensor(data).float();\n",
    "        output = torch.tensor(output).float()\n",
    "        self.data = data\n",
    "        self.output = output;\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx];\n",
    "        y = self.output[idx];\n",
    "\n",
    "        return x, y;\n",
    "\n",
    "# use the past 72 hours in advance and then predict the 1st hour, 6th hour, 12 hours!\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            running_loss += loss.item() * target.size(0)\n",
    "    return running_loss / len(val_loader.dataset)\n",
    "\n",
    "def Train_and_Evaluate(train_loader, val_loader, device, params1, params2, params3, numEpochs, early_stop_epochs):\n",
    "    model = BayesianModel(input_size = 12, params1 = params1, params2 = params2, params3 = params3, num_layers = 3, output_size = 1)\n",
    "    model = model.to(device);\n",
    "    LossFunction = torch.nn.L1Loss();\n",
    "    best_val_loss = float('inf')\n",
    "    Training_Loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "\n",
    "\n",
    "    Optimizer = torch.optim.Adam(params = model.parameters())\n",
    "    for epoch in range(0,numEpochs):\n",
    "        model.train()\n",
    "        Training_Loss = 0;\n",
    "        total_samples = 0;\n",
    "        for input, output in train_loader:\n",
    "            input = input.to(device);\n",
    "            #output = torch.squeeze(output, 1);\n",
    "            output = output.to(device);\n",
    "            predictedVal = model(input)\n",
    "            predictedVal = torch.squeeze(predictedVal, 1)\n",
    "            Optimizer.zero_grad();\n",
    "            batchLoss = LossFunction(predictedVal, output);\n",
    "            batchLoss.backward();\n",
    "            Optimizer.step();\n",
    "            Training_Loss += batchLoss * output.size(0) #* output.size(0);\n",
    "            total_samples += output.size(0)\n",
    "        Training_Loss = Training_Loss.item()/total_samples\n",
    "\n",
    "\n",
    "        Validation_Loss = 0;\n",
    "        print(\"passed \", epoch, \"epoch\", \"Training Loss: \", Training_Loss,\" \", end = \"\")\n",
    "        with torch.no_grad(): \n",
    "            model.eval() \n",
    "            total_val_samples = 0 \n",
    "            Validation_Loss = 0 \n",
    "            for val_input, val_output in val_loader:\n",
    "                val_input = val_input.to(device)\n",
    "                #val_output = torch.squeeze(val_output,1);\n",
    "                val_output = val_output.to(device)\n",
    "                predictedVal = model(val_input)\n",
    "                predictedVal = torch.squeeze(predictedVal, 1)\n",
    "                Validation_Loss += LossFunction(val_output, predictedVal) * val_output.size(0)\n",
    "                total_val_samples += val_output.size(0)\n",
    "            Validation_Loss = Validation_Loss/total_val_samples\n",
    "            print(\"Validation Loss: \", Validation_Loss)\n",
    "\n",
    "            if Validation_Loss < best_val_loss:\n",
    "                best_val_loss = Validation_Loss\n",
    "                torch.save(model, \"/home/jik19004/FilesToRun/BayesianBiDirectional/BayesianLSTMLong\")\n",
    "                early_stop_count = 0;   \n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "            if early_stop_count >= early_stop_epochs:\n",
    "                return (Training_Loss, best_val_loss)\n",
    "    return (Training_Loss, best_val_loss)\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            running_loss += loss.item() * target.size(0)\n",
    "    return running_loss / len(val_loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate2(model, val_loader, criterion, criterion2, device):\n",
    "    num_experiments = 100\n",
    "    criterion2 = criterion2.to(device)\n",
    "    with torch.no_grad():\n",
    "        model.train()\n",
    "        total_val_samples = 0;\n",
    "        Validation_Loss_MAE = 0;\n",
    "        Validation_Loss_MAPE = 0;\n",
    "        predictions = []  \n",
    "        std_list = [] \n",
    "        for val_input, val_output in val_loader:\n",
    "            val_input = val_input.to(device);\n",
    "            val_output = val_output.to(device);\n",
    "            #Avgpred\n",
    "            pred = model(val_input)\n",
    "            pred = torch.squeeze(pred, 1)\n",
    "            pred = torch.unsqueeze(pred, 0)\n",
    "               \n",
    "            for i in range(num_experiments - 1):\n",
    "                predictedVal2 = model(val_input)\n",
    "                predictedVal2 = torch.squeeze(predictedVal2, 1)\n",
    "                predictedVal2 = torch.unsqueeze(predictedVal2, 0)\n",
    "                pred = torch.cat([pred, predictedVal2], dim = 0)\n",
    "                \n",
    "            Avgpred = torch.mean(pred, dim = 0)\n",
    "            stdev = torch.std(pred, dim = 0)\n",
    "            predCsv = Avgpred.cpu().numpy()\n",
    "            stdev = stdev.cpu().numpy()\n",
    "            predictions.extend(predCsv) \n",
    "            std_list.extend(stdev)\n",
    "            \n",
    "            Validation_Loss_MAE += criterion(val_output, Avgpred) * val_output.size(0)\n",
    "            Validation_Loss_MAPE += criterion2(val_output, Avgpred) * val_output.size(0)\n",
    "            total_val_samples += val_output.size(0)\n",
    "        Validation_Loss_MAE = Validation_Loss_MAE/total_val_samples\n",
    "        Validation_Loss_MAPE = Validation_Loss_MAPE/total_val_samples \n",
    "        return Validation_Loss_MAE, Validation_Loss_MAPE, predictions, std_list   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_and_Evaluate2(train_loader, val_loader, device, params1, params2, numEpochs, early_stop_epochs):\n",
    "    #num_layers, input_dim, hidden_unit1, hidden_unit2, output_unit, lastNeurons, batch_size, params, device = None\n",
    "    model = BayesianModel(params1 = params1, params2 = params2, device = device)\n",
    "    model = model.to(device);\n",
    "    TrainEpochLoss = [] \n",
    "    ValidationEpochLoss = [] \n",
    "    LossFunction = torch.nn.L1Loss();\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "\n",
    "\n",
    "    Optimizer = torch.optim.Adam(params = model.parameters())\n",
    "    for epoch in range(0,numEpochs):\n",
    "        model.train()\n",
    "        Training_Loss = 0;\n",
    "        total_samples = 0;\n",
    "        for input, output in train_loader:\n",
    "            input = input.to(device);\n",
    "            output = torch.squeeze(output, 1);\n",
    "            output = output.to(device);\n",
    "            predictedVal = model(input)\n",
    "            predictedVal = torch.squeeze(predictedVal, 1)\n",
    "            Optimizer.zero_grad();\n",
    "            batchLoss = LossFunction(predictedVal, output);\n",
    "            batchLoss.backward();\n",
    "            Optimizer.step();\n",
    "            Training_Loss += batchLoss * output.size(0) #* output.size(0);\n",
    "            total_samples += output.size(0)\n",
    "        Training_Loss = Training_Loss.item()/total_samples\n",
    "        TrainEpochLoss.append(Training_Loss)\n",
    "\n",
    "\n",
    "        Validation_Loss = 0;\n",
    "        print(\"passed \", epoch, \"epoch\", \"Training Loss: \", Training_Loss,\" \", end = \"\")\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            total_val_samples = 0;\n",
    "            Validation_Loss = 0;\n",
    "            for val_input, val_output in val_loader:\n",
    "                val_input = val_input.to(device);\n",
    "                val_output = torch.squeeze(val_output,1);\n",
    "                val_output = val_output.to(device);\n",
    "                predictedVal = model(val_input)\n",
    "                predictedVal = torch.squeeze(predictedVal, 1)\n",
    "                Validation_Loss += LossFunction(val_output, predictedVal) * val_output.size(0)\n",
    "                total_val_samples += val_output.size(0)\n",
    "            Validation_Loss = Validation_Loss.item()/total_val_samples\n",
    "            print(\"Validation Loss: \", Validation_Loss)\n",
    "            ValidationEpochLoss.append(Validation_Loss)\n",
    "\n",
    "            if Validation_Loss < best_val_loss:\n",
    "                best_val_loss = Validation_Loss\n",
    "                torch.save(model, \"/home/jik19004/FilesToRun/BayesianBiDirectional/BayesianLSTMLong\")\n",
    "                early_stop_count = 0;   \n",
    "            else:\n",
    "                early_stop_count +=1\n",
    "            if early_stop_count >= early_stop_epochs:\n",
    "                return (TrainEpochLoss, ValidationEpochLoss);\n",
    "    return (TrainEpochLoss, ValidationEpochLoss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingDataset = TimeSeriesDataset(np.array(TrainingData),np.array(TrainingOutput));\n",
    "TrainingLoader = DataLoader(TrainingDataset, batch_size = 256);\n",
    "\n",
    "\n",
    "ValidationDataset = TimeSeriesDataset(ValidationData, ValidationOutput); ### Set it with the previous validation data\n",
    "ValidationLoader = DataLoader(ValidationDataset, batch_size = 256);\n",
    "\n",
    "\n",
    "TestingDataset = TimeSeriesDataset(TestingData,TestingOutput); ### Set it with the previous testing data.\n",
    "TestingLoader = DataLoader(TestingDataset, batch_size = 256);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = [3, 1, 3, 64, 1]\n",
    "params2 = [0.15, 0.15]\n",
    "params3 = [64, 32, 16]\n",
    "numEpochs = 100 \n",
    "early_stop_epochs = 1\n",
    "#Train_and_Evaluate(TrainingLoader,ValidationLoader, torch.device(\"cuda\"), params1, params2, params3, numEpochs, early_stop_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1/2019 6:00-1/2/2019 5:00\n",
      "1/2/2019 6:00-1/3/2019 5:00\n",
      "1/3/2019 6:00-1/4/2019 5:00\n",
      "1/4/2019 6:00-1/5/2019 5:00\n",
      "1/5/2019 6:00-1/6/2019 5:00\n",
      "1/6/2019 6:00-1/7/2019 5:00\n",
      "1/7/2019 6:00-1/8/2019 5:00\n",
      "1/8/2019 6:00-1/9/2019 5:00\n",
      "1/9/2019 6:00-1/10/2019 5:00\n",
      "1/10/2019 6:00-1/11/2019 5:00\n",
      "1/11/2019 6:00-1/12/2019 5:00\n",
      "1/12/2019 6:00-1/13/2019 5:00\n",
      "1/13/2019 6:00-1/14/2019 5:00\n",
      "1/14/2019 6:00-1/15/2019 5:00\n",
      "1/15/2019 6:00-1/16/2019 5:00\n",
      "1/16/2019 6:00-1/17/2019 5:00\n",
      "1/17/2019 6:00-1/18/2019 5:00\n",
      "1/18/2019 6:00-1/19/2019 5:00\n",
      "1/19/2019 6:00-1/20/2019 5:00\n",
      "1/20/2019 6:00-1/21/2019 5:00\n",
      "1/21/2019 6:00-1/22/2019 5:00\n",
      "1/22/2019 6:00-1/23/2019 5:00\n",
      "1/23/2019 6:00-1/24/2019 5:00\n",
      "1/24/2019 6:00-1/25/2019 5:00\n",
      "1/25/2019 6:00-1/26/2019 5:00\n",
      "1/26/2019 6:00-1/27/2019 5:00\n",
      "1/27/2019 6:00-1/28/2019 5:00\n",
      "1/28/2019 6:00-1/29/2019 5:00\n",
      "1/29/2019 6:00-1/30/2019 5:00\n",
      "1/30/2019 6:00-1/31/2019 5:00\n",
      "1/31/2019 6:00-2/1/2019 5:00\n",
      "2/1/2019 6:00-2/2/2019 5:00\n",
      "2/2/2019 6:00-2/3/2019 5:00\n",
      "2/3/2019 6:00-2/4/2019 5:00\n",
      "2/4/2019 6:00-2/5/2019 5:00\n",
      "2/5/2019 6:00-2/6/2019 5:00\n",
      "2/6/2019 6:00-2/7/2019 5:00\n",
      "2/7/2019 6:00-2/8/2019 5:00\n",
      "2/8/2019 6:00-2/9/2019 5:00\n",
      "2/9/2019 6:00-2/10/2019 5:00\n",
      "2/10/2019 6:00-2/11/2019 5:00\n",
      "2/11/2019 6:00-2/12/2019 5:00\n",
      "2/12/2019 6:00-2/13/2019 5:00\n",
      "2/13/2019 6:00-2/14/2019 5:00\n",
      "2/14/2019 6:00-2/15/2019 5:00\n",
      "2/15/2019 6:00-2/16/2019 5:00\n",
      "2/16/2019 6:00-2/17/2019 5:00\n",
      "2/17/2019 6:00-2/18/2019 5:00\n",
      "2/18/2019 6:00-2/19/2019 5:00\n",
      "2/19/2019 6:00-2/20/2019 5:00\n",
      "2/20/2019 6:00-2/21/2019 5:00\n",
      "2/21/2019 6:00-2/22/2019 5:00\n",
      "2/22/2019 6:00-2/23/2019 5:00\n",
      "2/23/2019 6:00-2/24/2019 5:00\n",
      "2/24/2019 6:00-2/25/2019 5:00\n",
      "2/25/2019 6:00-2/26/2019 5:00\n",
      "2/26/2019 6:00-2/27/2019 5:00\n",
      "2/27/2019 6:00-2/28/2019 5:00\n",
      "2/28/2019 6:00-3/1/2019 5:00\n",
      "3/1/2019 6:00-3/2/2019 5:00\n",
      "3/2/2019 6:00-3/3/2019 5:00\n",
      "3/3/2019 6:00-3/4/2019 5:00\n",
      "3/4/2019 6:00-3/5/2019 5:00\n",
      "3/5/2019 6:00-3/6/2019 5:00\n",
      "3/6/2019 6:00-3/7/2019 5:00\n",
      "3/7/2019 6:00-3/8/2019 5:00\n",
      "3/8/2019 6:00-3/9/2019 5:00\n",
      "3/9/2019 6:00-3/10/2019 5:00\n",
      "3/10/2019 6:00-3/11/2019 5:00\n",
      "3/11/2019 6:00-3/12/2019 5:00\n",
      "3/12/2019 6:00-3/13/2019 5:00\n",
      "3/13/2019 6:00-3/14/2019 5:00\n",
      "3/14/2019 6:00-3/15/2019 5:00\n",
      "3/15/2019 6:00-3/16/2019 5:00\n",
      "3/16/2019 6:00-3/17/2019 5:00\n",
      "3/17/2019 6:00-3/18/2019 5:00\n",
      "3/18/2019 6:00-3/19/2019 5:00\n",
      "3/19/2019 6:00-3/20/2019 5:00\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.regression import MeanAbsolutePercentageError\n",
    "\n",
    "def LongTermEvaluate(model, ValidationData, ValidationOutput, DateTimeCol = DateTimeCol, index_start = 70117, index_end = 91291):\n",
    "    Validation_Loss_MAE = []\n",
    "    Validation_Loss_MAPE = []\n",
    "    df_val = pd.DataFrame() \n",
    "    df_val_std = pd.DataFrame()\n",
    "    \n",
    "    for i in range(index_start, index_end, 24):       \n",
    "        val_start = i\n",
    "        val_end = val_start + 28 #val_start + 28\n",
    "        val_output_start = DateTimeCol[val_start + 6]\n",
    "        val_output_end = DateTimeCol[val_end + 1]\n",
    "\n",
    "        if val_end < index_end:            \n",
    "            ValidationDataset = TimeSeriesDataset(np.array(ValidationData[val_start - index_start: val_start - index_start + 24]), \n",
    "                                               np.array(ValidationOutput[val_start - index_start: val_start - index_start + 24]))\n",
    "            ValidationLoader = DataLoader(ValidationDataset, batch_size = 3, shuffle = False) \n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "            val_str = val_output_start + \"-\" + val_output_end           \n",
    "            print(val_str) \n",
    "            val_loss = evaluate2(model, ValidationLoader, torch.nn.L1Loss(),MeanAbsolutePercentageError(), device)            \n",
    "            Validation_Loss_MAE.append(val_loss[0].item())\n",
    "            Validation_Loss_MAPE.append(val_loss[1].item())\n",
    "            df_val = pd.concat([df_val, pd.DataFrame({val_str: val_loss[2]})], ignore_index=False, axis=1)\n",
    "            df_val_std = pd.concat([df_val_std, pd.DataFrame({val_str: val_loss[3]})], ignore_index = False, axis =1 )\n",
    "        else:\n",
    "            return Validation_Loss_MAE, Validation_Loss_MAPE, df_val, df_val_std \n",
    "         \n",
    "model = torch.load(\"/home/jik19004/FilesToRun/BayesianBiDirectional/BayesianBiLSTMLong\")\n",
    "\n",
    "WeatherData = pd.read_csv('/home/jik19004/FilesToRun/ASOS_10_CT_stations_tmpc_demand_2011_2023.csv').drop(columns = [\"Unnamed: 0\"])\n",
    "DateTimeCol = WeatherData[\"Datetime\"]\n",
    "\n",
    "tuples = LongTermEvaluate(model, ValidationData, ValidationOutput, DateTimeCol = DateTimeCol, index_start = 70123, index_end = 72000)\n",
    "\n",
    "ValidationLoss_series = pd.DataFrame({\"Validation_MAE\": tuples[0], \"Validation_MAPE\": tuples[1]})\n",
    "ValidationLoss_series.to_csv(\"/home/jik19004/FilesToRun/BayesianBiDirectional/ValidationLossesLong.csv\", index = False)\n",
    "tuples[2].to_csv(\"/home/jik19004/FilesToRun/BayesianBiDirectional/ValidationPredictionsLong.csv\", index = False)\n",
    "tuples[3].to_csv(\"/home/jik19004/FilesToRun/BayesianBiDirectional/ValidationStdLong.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Validation_MAE</th>\n",
       "      <th>Validation_MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2804.074219</td>\n",
       "      <td>26.449173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3305.465576</td>\n",
       "      <td>29.970320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3238.155029</td>\n",
       "      <td>30.488617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3164.583496</td>\n",
       "      <td>30.716185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3037.572266</td>\n",
       "      <td>26.617725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2787.774414</td>\n",
       "      <td>25.242498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2664.345703</td>\n",
       "      <td>23.904228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2804.362305</td>\n",
       "      <td>25.244831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>3091.761719</td>\n",
       "      <td>28.424679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>3080.515137</td>\n",
       "      <td>28.250755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Validation_MAE  Validation_MAPE\n",
       "0      2804.074219        26.449173\n",
       "1      3305.465576        29.970320\n",
       "2      3238.155029        30.488617\n",
       "3      3164.583496        30.716185\n",
       "4      3037.572266        26.617725\n",
       "..             ...              ...\n",
       "73     2787.774414        25.242498\n",
       "74     2664.345703        23.904228\n",
       "75     2804.362305        25.244831\n",
       "76     3091.761719        28.424679\n",
       "77     3080.515137        28.250755\n",
       "\n",
       "[78 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"/home/jik19004/FilesToRun/BayesianBiDirectional/ValidationLossesLong.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
