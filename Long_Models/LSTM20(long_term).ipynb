{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu102\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>BDL_tmpc</th>\n",
       "      <th>BDR_tmpc</th>\n",
       "      <th>DXR_tmpc</th>\n",
       "      <th>GON_tmpc</th>\n",
       "      <th>HFD_tmpc</th>\n",
       "      <th>HVN_tmpc</th>\n",
       "      <th>IJD_tmpc</th>\n",
       "      <th>MMK_tmpc</th>\n",
       "      <th>OXC_tmpc</th>\n",
       "      <th>SNC_tmpc</th>\n",
       "      <th>Demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2011 0:00</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.78</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3053.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/2011 1:00</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.22</td>\n",
       "      <td>3.33</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2892.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/2011 2:00</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>2.78</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.78</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>0.56</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2774.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/2011 3:00</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>3.89</td>\n",
       "      <td>2.22</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/2011 4:00</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>1.67</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2698.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Datetime  BDL_tmpc  BDR_tmpc  DXR_tmpc  GON_tmpc  HFD_tmpc  HVN_tmpc  \\\n",
       "0  1/1/2011 0:00      1.11      1.11      0.00      2.78      1.67      1.11   \n",
       "1  1/1/2011 1:00      1.11      2.22     -0.56      3.33      2.22      3.33   \n",
       "2  1/1/2011 2:00     -0.56      2.78     -1.67      3.33      2.78      2.78   \n",
       "3  1/1/2011 3:00     -1.11      2.22     -1.11      3.89      2.22      1.11   \n",
       "4  1/1/2011 4:00     -1.67      1.67     -1.11      3.33      2.22     -0.56   \n",
       "\n",
       "   IJD_tmpc  MMK_tmpc  OXC_tmpc  SNC_tmpc  Demand  \n",
       "0     -2.78     -1.67       3.0       2.0  3053.0  \n",
       "1     -2.78     -1.67       3.0       3.0  2892.0  \n",
       "2     -2.22      0.56       3.0       3.0  2774.0  \n",
       "3     -2.78      0.56       2.0       4.0  2710.0  \n",
       "4     -2.78     -1.67       4.0       5.0  2698.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "data = pd.read_csv(\"/home/jik19004/FilesToRun/ASOS_10_CT_stations_tmpc_demand_2011_2023.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "WeatherData = pd.read_csv('/home/jik19004/FilesToRun/ASOS_10_CT_stations_tmpc_demand_2011_2023.csv').drop(columns = [\"Unnamed: 0\"])\n",
    "WeatherData.ffill(inplace = True)\n",
    "WeatherData.bfill(inplace = True) # fill in missing values with the previous value.\n",
    "DateTimeCol = WeatherData[\"Datetime\"]\n",
    "HourCol = []\n",
    "WeekDayorWeekEndCol = [] \n",
    "\n",
    "for date in DateTimeCol:\n",
    "    date = datetime.strptime(date, \"%m/%d/%Y %H:%M\")\n",
    "    HourCol.append(date.hour)\n",
    "    if date.weekday() < 5:\n",
    "        WeekDayorWeekEndCol.append(0)\n",
    "    else:\n",
    "        WeekDayorWeekEndCol.append(1)\n",
    "\n",
    "WeatherData.drop(columns = [\"Datetime\"], inplace = True) # drop the datetime column. \n",
    "\n",
    "\n",
    "WeatherData.insert(0, \"Hour\", HourCol)\n",
    "WeatherData.insert(1, \"Weekday or Weekend\", WeekDayorWeekEndCol)\n",
    "DateTimeCol = [datetime.strptime(date, \"%m/%d/%Y %H:%M\") for date in DateTimeCol]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52603\n",
      "70122\n",
      "70123\n",
      "78882\n",
      "78883\n",
      "96403\n"
     ]
    }
   ],
   "source": [
    "train_start = \"1/1/2017 0:00\"\n",
    "train_end = \"12/31/2018 23:00\"\n",
    "\n",
    "val_start = \"1/1/2019 0:00\"\n",
    "val_end = \"12/31/2019 23:00\"\n",
    "\n",
    "test_start = \"1/01/2020 0:00\"\n",
    "test_end = \"12/31/2021 0:00\"\n",
    "\n",
    "\n",
    "date_train_start = datetime.strptime(train_start, \"%m/%d/%Y %H:%M\")\n",
    "date_train_end = datetime.strptime(train_end, \"%m/%d/%Y %H:%M\")\n",
    "\n",
    "date_val_start = datetime.strptime(val_start, \"%m/%d/%Y %H:%M\")\n",
    "date_val_end = datetime.strptime(val_end, \"%m/%d/%Y %H:%M\")\n",
    "\n",
    "date_test_start = datetime.strptime(test_start, \"%m/%d/%Y %H:%M\")\n",
    "date_test_end = datetime.strptime(test_end, \"%m/%d/%Y %H:%M\")\n",
    "\n",
    "DateTimeCol_num = np.array(DateTimeCol)\n",
    "\n",
    "print(np.where(DateTimeCol_num == date_train_start)[0][0])\n",
    "print(np.where(DateTimeCol_num == date_train_end)[0][0])\n",
    "\n",
    "\n",
    "print(np.where(DateTimeCol_num == date_val_start)[0][0])\n",
    "print(np.where(DateTimeCol_num == date_val_end)[0][0])\n",
    "\n",
    "print(np.where(DateTimeCol_num == date_test_start)[0][0])\n",
    "print(np.where(DateTimeCol_num == date_test_end)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index for Jan 1, 2017:  52603\n",
      "index for Dec 31, 2021:  70121\n",
      "index for Jan 1, 2019:  70123\n",
      "index for Dec 31, 2019  78881\n",
      "index for Jan 1, 2019:  78883\n",
      "index for Dec 31, 2019  91289\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(DateTimeCol)):\n",
    "    date = DateTimeCol[i]\n",
    "    if int(date.year == 2017) and int(date.month) == 1 and int(date.day) == 1 and int(date.hour) == 0: # \n",
    "        print(\"index for Jan 1, 2017: \", i)\n",
    "    if int(date.year) == 2018 and int(date.month) == 12 and int(date.day) == 31 and int(date.hour) == 22:\n",
    "        print(\"index for Dec 31, 2021: \", i)\n",
    "    if int(date.year) == 2019 and int(date.month) == 1 and int(date.day) == 1 and int(date.hour) == 0: # \n",
    "        print(\"index for Jan 1, 2019: \", i)\n",
    "    if int(date.year) == 2019 and int(date.month) == 12 and int(date.day) == 31 and int(date.hour) == 22:\n",
    "        print(\"index for Dec 31, 2019 \", i)\n",
    "    if int(date.year) == 2020 and int(date.month) == 1 and int(date.day) == 1 and int(date.hour) == 0: # \n",
    "        print(\"index for Jan 1, 2019: \", i)\n",
    "    if int(date.year) == 2021 and int(date.month) == 5 and int(date.day) == 31 and int(date.hour) == 22: \n",
    "        print(\"index for Dec 31, 2019 \", i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_sequences(data, outputData, input_n_steps, output_n_steps):\n",
    "    X = []\n",
    "    Y = []\n",
    "    length = len(data)\n",
    "    print(length)\n",
    "    for i in range(0,length, 1):\n",
    "        input_indx = i + input_n_steps\n",
    "        #output_indx = input_indx + output_n_steps\n",
    "        output_indx = i + output_n_steps\n",
    "        if (input_indx >= len(data)): # we need to have equally split sequences.\n",
    "            break               # The remaining data that cannot fit into a fixed\n",
    "                                # sequence will immediately be cut!\n",
    "        else:\n",
    "            Xsample = data.iloc[i:input_indx, :] # get the previous data\n",
    "            #Ysample = outputData[input_indx:output_indx]\n",
    "            Ysample = outputData[i]\n",
    "            X.append(Xsample)\n",
    "            Y.append(Ysample)\n",
    "    X = np.asarray(X).astype('float64')\n",
    "    Y = np.asarray(Y).astype('float64')\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "def splitDataAndScale(data, output, train_start = None, train_end = None, val_start = None, val_end = None, test_start = None, test_end = None):\n",
    "    #train_end +=1\n",
    "    #val_end += 1\n",
    "    #test_end += 1\n",
    "    TrainingData = (data.iloc[train_start: train_end + 1, :].copy())\n",
    "    print(len(TrainingData))\n",
    "    TrainingCategories = TrainingData.iloc[:, [0,1]]\n",
    "    TrainingNumerical = TrainingData.iloc[:, 2:]\n",
    "    TrainingOutput = output[train_start + 6: train_end + 2].copy()  \n",
    "    Scaler = StandardScaler().fit(TrainingNumerical)\n",
    "    TrainingNumerical = Scaler.transform(TrainingNumerical)\n",
    "    TrainingCategories = TrainingCategories.reset_index(drop = True)\n",
    "    TrainingData = pd.concat([TrainingCategories, pd.DataFrame(TrainingNumerical)], axis = 1)\n",
    "    TrainingData.reset_index(drop = True, inplace = True)\n",
    "    TrainingOutput.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    ValidationData = data.iloc[val_start: val_end + 1, :].copy()\n",
    "    ValidationData.reset_index(drop = True, inplace = True)\n",
    "    ValidationCategories = ValidationData.iloc[:, [0,1]]\n",
    "    ValidationNumerical = ValidationData.iloc[:, 2:]\n",
    "    ValidationNumerical = Scaler.transform(ValidationNumerical)\n",
    "    ValidationCategories = ValidationCategories.reset_index(drop = True)\n",
    "    ValidationData = pd.concat([ValidationCategories, pd.DataFrame(ValidationNumerical)], axis = 1)\n",
    "    ValidationOutput = output[val_start + 6: val_end + 2].copy()\n",
    "    ValidationData.reset_index(drop = True, inplace = True)\n",
    "    ValidationOutput.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    TestingData = data.iloc[test_start: test_end + 1, :].copy()\n",
    "    TestingData.reset_index(drop = True, inplace = True)\n",
    "    TestingCategories = TestingData.iloc[:, [0,1]]\n",
    "    TestingNumerical = TestingData.iloc[:, 2:]\n",
    "    TestingNumerical = Scaler.transform(TestingNumerical)\n",
    "    TestingCategories = TestingCategories.reset_index(drop = True)\n",
    "    TestingData = pd.concat([TestingCategories, pd.DataFrame(TestingNumerical)], axis = 1)\n",
    "    TestingOutput = output[test_start + 6: test_end + 2].copy()\n",
    "    TestingData.reset_index(drop = True, inplace = True)\n",
    "    TestingOutput.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "    TrainingSequences = return_sequences(TrainingData, TrainingOutput, 6, 1)\n",
    "\n",
    "    TransformedTrainingData = TrainingSequences[0]\n",
    "    TransformedTrainingOutput = TrainingSequences[1]\n",
    "\n",
    "    ValidationSequences = return_sequences(ValidationData, ValidationOutput, 6, 1)\n",
    "\n",
    "    TransformedValidationData = ValidationSequences[0]\n",
    "    TransformedValidationOutput = ValidationSequences[1]\n",
    "\n",
    "    TestingSequences = return_sequences(TestingData, TestingOutput, 6, 1)\n",
    "\n",
    "    TransformedTestingData = TestingSequences[0]\n",
    "    TransformedTestingOutput = TestingSequences[1]\n",
    "\n",
    "\n",
    "    return (TransformedTrainingData, TransformedTrainingOutput, TransformedValidationData, TransformedValidationOutput,\n",
    "    TransformedTestingData, TransformedTestingOutput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17519\n",
      "17519\n",
      "8759\n",
      "12407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "DemandData = WeatherData['Demand'].copy() # The output data\n",
    "WeatherData.drop(columns = ['Demand'], inplace = True)\n",
    "data = splitDataAndScale(WeatherData, DemandData, train_start = 52603, train_end = 70121, val_start = 70123, val_end = 78881, test_start = 78883, test_end = 91289) # splitting the data into training, validation, and testing.\n",
    "\n",
    "\n",
    "TrainingData = data[0]\n",
    "TrainingOutput = data[1]\n",
    "\n",
    "ValidationData = data[2]\n",
    "ValidationOutput = data[3]\n",
    "\n",
    "TestingData = data[4]\n",
    "TestingOutput = data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17513, 6, 12)\n",
      "(8753, 6, 12)\n",
      "(12401, 6, 12)\n"
     ]
    }
   ],
   "source": [
    "print(TrainingData.shape)\n",
    "print(ValidationData.shape)\n",
    "print(TestingData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import PackedSequence\n",
    "from typing import *\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, params1, params2, params3, num_layers, output_size):    \n",
    "    #params1 = [conv_kernel_size, stride, max_kernel_size, LSTM_hidden_size, LSTM_num_layers]\n",
    "    #params2 = [dropout_LSTM, dropout_FFN]\n",
    "    #params3 = [hidden_size1, hidden_size2, hidden_size3]\n",
    "        super(LSTMModel, self).__init__()        \n",
    "        self.conv1D = torch.nn.Conv1d(in_channels = 18, out_channels = 18, kernel_size = params1[0], stride = params1[1])\n",
    "        self.max1D = torch.nn.MaxPool1d(kernel_size = params1[2], stride = params1[1])\n",
    "        self.dropout1 = torch.nn.Dropout(params2[0])\n",
    "        self.input_size = input_size - params1[0] - params1[2] + 2\n",
    "        self.BiLSTM = nn.LSTM(input_size = self.input_size, hidden_size = params1[3], bidirectional = False, num_layers = params1[4])\n",
    "        \n",
    "        layers = []\n",
    "        input_size = params1[3] #params1[3] *2 \n",
    "        num_units = 0 \n",
    "        for i in range(num_layers):\n",
    "            num_units = params3[i] \n",
    "            layers.append(torch.nn.Linear(input_size, num_units, bias = True))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(params2[0])) #also add in the dropout. \n",
    "            input_size = num_units\n",
    "        \n",
    "        self.intermediate_layers = torch.nn.Sequential(*layers)\n",
    "        self.finalNN = torch.nn.Linear(num_units, output_size, bias = True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1D(x)\n",
    "        x = self.max1D(x) \n",
    "        x = self.dropout1(x) \n",
    "        x = self.BiLSTM(x) \n",
    "        \n",
    "        x = x[0]\n",
    "        x = x[:, -1,:]\n",
    "        #x = x[1][0]\n",
    "        \n",
    "        x = self.intermediate_layers(x) \n",
    "        x = self.finalNN(x) \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (conv1D): Conv1d(18, 18, kernel_size=(3,), stride=(1,))\n",
      "  (max1D): MaxPool1d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout1): Dropout(p=0.15, inplace=False)\n",
      "  (BiLSTM): LSTM(8, 64)\n",
      "  (intermediate_layers): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.15, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.15, inplace=False)\n",
      "    (6): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.15, inplace=False)\n",
      "  )\n",
      "  (finalNN): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"/home/jik19004/FilesToRun/BayesianBiDirectional/Long_Models/LSTMLong\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, output):\n",
    "        data = torch.tensor(data).float();\n",
    "        output = torch.tensor(output).float()\n",
    "        self.data = data\n",
    "        self.output = output;\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx];\n",
    "        y = self.output[idx];\n",
    "\n",
    "        return x, y;\n",
    "\n",
    "# use the past 72 hours in advance and then predict the 1st hour, 6th hour, 12 hours!\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            running_loss += loss.item() * target.size(0)\n",
    "    return running_loss / len(val_loader.dataset)\n",
    "\n",
    "def Train_and_Evaluate(train_loader, val_loader, device, params1, params2, params3, numEpochs, early_stop_epochs):\n",
    "    model = LSTMModel(input_size = 12, params1 = params1, params2 = params2, params3 = params3, num_layers = 3, output_size = 1)\n",
    "    model = model.to(device);\n",
    "    LossFunction = torch.nn.L1Loss();\n",
    "    best_val_loss = float('inf')\n",
    "    Training_Loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "\n",
    "\n",
    "    Optimizer = torch.optim.Adam(params = model.parameters())\n",
    "    for epoch in range(0,numEpochs):\n",
    "        model.train()\n",
    "        Training_Loss = 0;\n",
    "        total_samples = 0;\n",
    "        for input, output in train_loader:\n",
    "            input = input.to(device);\n",
    "            #output = torch.squeeze(output, 1);\n",
    "            output = output.to(device);\n",
    "            predictedVal = model(input)\n",
    "            predictedVal = torch.squeeze(predictedVal, 1)\n",
    "            Optimizer.zero_grad();\n",
    "            batchLoss = LossFunction(predictedVal, output);\n",
    "            batchLoss.backward();\n",
    "            Optimizer.step();\n",
    "            Training_Loss += batchLoss * output.size(0) #* output.size(0);\n",
    "            total_samples += output.size(0)\n",
    "        Training_Loss = Training_Loss.item()/total_samples\n",
    "\n",
    "\n",
    "        Validation_Loss = 0;\n",
    "        print(\"passed \", epoch, \"epoch\", \"Training Loss: \", Training_Loss,\" \", end = \"\")\n",
    "        with torch.no_grad(): \n",
    "            model.eval() \n",
    "            total_val_samples = 0 \n",
    "            Validation_Loss = 0 \n",
    "            for val_input, val_output in val_loader:\n",
    "                val_input = val_input.to(device)\n",
    "                #val_output = torch.squeeze(val_output,1);\n",
    "                val_output = val_output.to(device)\n",
    "                predictedVal = model(val_input)\n",
    "                predictedVal = torch.squeeze(predictedVal, 1)\n",
    "                Validation_Loss += LossFunction(val_output, predictedVal) * val_output.size(0)\n",
    "                total_val_samples += val_output.size(0)\n",
    "            Validation_Loss = Validation_Loss/total_val_samples\n",
    "            print(\"Validation Loss: \", Validation_Loss)\n",
    "\n",
    "            if Validation_Loss < best_val_loss:\n",
    "                best_val_loss = Validation_Loss\n",
    "                torch.save(model, \"LSTMLong\")\n",
    "                early_stop_count = 0;   \n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "            if early_stop_count >= early_stop_epochs:\n",
    "                return (Training_Loss, best_val_loss)\n",
    "    return (Training_Loss, best_val_loss)\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            running_loss += loss.item() * target.size(0)\n",
    "    return running_loss / len(val_loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate2(model, val_loader, criterion, criterion2, device):\n",
    "    num_experiments = 1\n",
    "    criterion2 = criterion2.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_val_samples = 0;\n",
    "        Validation_Loss_MAE = 0;\n",
    "        Validation_Loss_MAPE = 0;\n",
    "        predictions = []  \n",
    "        std_list = [] \n",
    "        for val_input, val_output in val_loader:\n",
    "            val_input = val_input.to(device);\n",
    "            val_output = val_output.to(device);\n",
    "            #Avgpred\n",
    "            pred = model(val_input)\n",
    "            pred = torch.squeeze(pred, 1)\n",
    "            pred = torch.unsqueeze(pred, 0)\n",
    "               \n",
    "            for i in range(num_experiments - 1):\n",
    "                predictedVal2 = model(val_input)\n",
    "                predictedVal2 = torch.squeeze(predictedVal2, 1)\n",
    "                predictedVal2 = torch.unsqueeze(predictedVal2, 0)\n",
    "                pred = torch.cat([pred, predictedVal2], dim = 0)\n",
    "            \n",
    "            Avgpred = torch.mean(pred, dim = 0)\n",
    "            stdev = torch.std(pred, dim = 0)\n",
    "            predCsv = Avgpred.cpu().numpy()\n",
    "            stdev = stdev.cpu().numpy()\n",
    "            predictions.extend(predCsv) \n",
    "            \n",
    "            Validation_Loss_MAE += criterion(val_output, Avgpred) * val_output.size(0)\n",
    "            Validation_Loss_MAPE += criterion2(val_output, Avgpred) * val_output.size(0)\n",
    "            total_val_samples += val_output.size(0)\n",
    "        Validation_Loss_MAE = Validation_Loss_MAE/total_val_samples\n",
    "        Validation_Loss_MAPE = Validation_Loss_MAPE/total_val_samples \n",
    "        return Validation_Loss_MAE, Validation_Loss_MAPE, predictions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_and_Evaluate2(train_loader, val_loader, device, params1, params2, numEpochs, early_stop_epochs):\n",
    "    #num_layers, input_dim, hidden_unit1, hidden_unit2, output_unit, lastNeurons, batch_size, params, device = None\n",
    "    model = LSTMModel(params1 = params1, params2 = params2, device = device)\n",
    "    model = model.to(device);\n",
    "    TrainEpochLoss = [] \n",
    "    ValidationEpochLoss = [] \n",
    "    LossFunction = torch.nn.L1Loss();\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "\n",
    "\n",
    "    Optimizer = torch.optim.Adam(params = model.parameters())\n",
    "    for epoch in range(0,numEpochs):\n",
    "        model.train()\n",
    "        Training_Loss = 0;\n",
    "        total_samples = 0;\n",
    "        for input, output in train_loader:\n",
    "            input = input.to(device);\n",
    "            output = torch.squeeze(output, 1);\n",
    "            output = output.to(device);\n",
    "            predictedVal = model(input)\n",
    "            predictedVal = torch.squeeze(predictedVal, 1)\n",
    "            Optimizer.zero_grad();\n",
    "            batchLoss = LossFunction(predictedVal, output);\n",
    "            batchLoss.backward();\n",
    "            Optimizer.step();\n",
    "            Training_Loss += batchLoss * output.size(0) #* output.size(0);\n",
    "            total_samples += output.size(0)\n",
    "        Training_Loss = Training_Loss.item()/total_samples\n",
    "        TrainEpochLoss.append(Training_Loss)\n",
    "\n",
    "\n",
    "        Validation_Loss = 0;\n",
    "        print(\"passed \", epoch, \"epoch\", \"Training Loss: \", Training_Loss,\" \", end = \"\")\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            total_val_samples = 0;\n",
    "            Validation_Loss = 0;\n",
    "            for val_input, val_output in val_loader:\n",
    "                val_input = val_input.to(device);\n",
    "                val_output = torch.squeeze(val_output,1);\n",
    "                val_output = val_output.to(device);\n",
    "                predictedVal = model(val_input)\n",
    "                predictedVal = torch.squeeze(predictedVal, 1)\n",
    "                Validation_Loss += LossFunction(val_output, predictedVal) * val_output.size(0)\n",
    "                total_val_samples += val_output.size(0)\n",
    "            Validation_Loss = Validation_Loss.item()/total_val_samples\n",
    "            print(\"Validation Loss: \", Validation_Loss)\n",
    "            ValidationEpochLoss.append(Validation_Loss)\n",
    "\n",
    "            if Validation_Loss < best_val_loss:\n",
    "                best_val_loss = Validation_Loss\n",
    "                torch.save(model, \"LSTMLong\")\n",
    "                early_stop_count = 0;   \n",
    "            else:\n",
    "                early_stop_count +=1\n",
    "            if early_stop_count >= early_stop_epochs:\n",
    "                return (TrainEpochLoss, ValidationEpochLoss);\n",
    "    return (TrainEpochLoss, ValidationEpochLoss);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingDataset = TimeSeriesDataset(np.array(TrainingData),np.array(TrainingOutput));\n",
    "TrainingLoader = DataLoader(TrainingDataset, batch_size = 256);\n",
    "\n",
    "\n",
    "ValidationDataset = TimeSeriesDataset(ValidationData, ValidationOutput); ### Set it with the previous validation data\n",
    "ValidationLoader = DataLoader(ValidationDataset, batch_size = 256);\n",
    "\n",
    "\n",
    "TestingDataset = TimeSeriesDataset(TestingData,TestingOutput); ### Set it with the previous testing data.\n",
    "TestingLoader = DataLoader(TestingDataset, batch_size = 256);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tuples = LongTermEvaluate(model, ValidationData, ValidationOutput, DateTimeCol = DateTimeCol, index_start = 70123, index_end = 72000)\\n\\nValidationLoss_series = pd.DataFrame({\"Validation_MAE\": tuples[0], \"Validation_MAPE\": tuples[1]})\\nValidationLoss_series.to_csv(\"/home/jik19004/FilesToRun/BayesianBiDirectional/ValidationLossesLong.csv\", index = False)\\ntuples[2].to_csv(\"/home/jik19004/FilesToRun/BayesianBiDirectional/ValidationPredictionsLong.csv\", index = False)\\ntuples[3].to_csv(\"/home/jik19004/FilesToRun/BayesianBiDirectional/ValidationStdLong.csv\", index = False)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.regression import MeanAbsolutePercentageError\n",
    "\n",
    "def LongTermEvaluate(model, ValidationData, ValidationOutput, DateTimeCol = DateTimeCol, index_start = 70117, index_end = 91291):\n",
    "    Validation_Loss_MAE = []\n",
    "    Validation_Loss_MAPE = []\n",
    "    df_val = pd.DataFrame() \n",
    "    df_val_std = pd.DataFrame()\n",
    "    \n",
    "    for i in range(index_start, index_end, 24):       \n",
    "        val_start = i\n",
    "        val_end = val_start + 28 #val_start + 28\n",
    "        val_output_start = DateTimeCol[val_start + 6]\n",
    "        val_output_end = DateTimeCol[val_end + 1]\n",
    "\n",
    "        if val_end < index_end:            \n",
    "            ValidationDataset = TimeSeriesDataset(np.array(ValidationData[val_start - index_start: val_start - index_start + 24]), \n",
    "                                               np.array(ValidationOutput[val_start - index_start: val_start - index_start + 24]))\n",
    "            ValidationLoader = DataLoader(ValidationDataset, batch_size = 3, shuffle = False) \n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "            val_str = val_output_start + \"-\" + val_output_end           \n",
    "            print(val_str) \n",
    "            val_loss = evaluate2(model, ValidationLoader, torch.nn.L1Loss(),MeanAbsolutePercentageError(), device)            \n",
    "            Validation_Loss_MAE.append(val_loss[0].item())\n",
    "            Validation_Loss_MAPE.append(val_loss[1].item())\n",
    "            df_val = pd.concat([df_val, pd.DataFrame({val_str: val_loss[2]})], ignore_index=False, axis=1)\n",
    "            df_val_std = pd.concat([df_val_std, pd.DataFrame({val_str: val_loss[3]})], ignore_index = False, axis =1 )\n",
    "        else:\n",
    "            return Validation_Loss_MAE, Validation_Loss_MAPE, df_val, df_val_std \n",
    "         \n",
    "#model = torch.load(\"/home/jik19004/FilesToRun/BayesianBiDirectional/BayesianBiLSTMLong\")\n",
    "\n",
    "WeatherData = pd.read_csv('/home/jik19004/FilesToRun/ASOS_10_CT_stations_tmpc_demand_2011_2023.csv').drop(columns = [\"Unnamed: 0\"])\n",
    "DateTimeCol = WeatherData[\"Datetime\"]\n",
    "\n",
    "\"\"\"tuples = LongTermEvaluate(model, ValidationData, ValidationOutput, DateTimeCol = DateTimeCol, index_start = 70123, index_end = 72000)\n",
    "\n",
    "ValidationLoss_series = pd.DataFrame({\"Validation_MAE\": tuples[0], \"Validation_MAPE\": tuples[1]})\n",
    "ValidationLoss_series.to_csv(\"/home/jik19004/FilesToRun/BayesianBiDirectional/ValidationLossesLong.csv\", index = False)\n",
    "tuples[2].to_csv(\"/home/jik19004/FilesToRun/BayesianBiDirectional/ValidationPredictionsLong.csv\", index = False)\n",
    "tuples[3].to_csv(\"/home/jik19004/FilesToRun/BayesianBiDirectional/ValidationStdLong.csv\", index = False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0936849250696426\n",
      "0.049352038265458566\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../TestingLong/NormalTestingLossesLong.csv\")\n",
    "data2 = pd.read_csv(\"../TestingLong/TestingLossesLong.csv\")\n",
    "print(np.average(data[\"Testing_MAPE\"]))\n",
    "print(np.average(data2[\"Testing_MAPE\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
