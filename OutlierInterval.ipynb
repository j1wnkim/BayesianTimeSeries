{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "train_preds = pd.read_csv(\"TrainingLong/TrainingPredictionsLong_Gaussian.csv\")\n",
    "train_std = pd.read_csv(\"TrainingLong/TrainingStdLong_Gaussian.csv\")\n",
    "\n",
    "val_preds = pd.read_csv(\"ValidationLong/ValidationPredictionsLong_Gaussian.csv\")\n",
    "val_std = pd.read_csv(\"ValidationLong/ValidationStdLong_Gaussian.csv\")\n",
    "\n",
    "test_preds = pd.read_csv(\"TestingLong/TestingPredictionsLong_Gaussian.csv\")\n",
    "test_std = pd.read_csv(\"TestingLong/TestingStdLong_Gaussian.csv\")\n",
    "\n",
    "train_actual = pd.read_csv(\"TrainingLong/TrainingActual_Long.csv\")\n",
    "val_actual = pd.read_csv(\"ValidationLong/ValidationActual_Long.csv\")\n",
    "test_actual = pd.read_csv(\"TestingLong/TestingActual_Long.csv\")\n",
    "\n",
    "# Define a function to calculate the encapsulation percentage\n",
    "def calculate_encapsulation_percentage(confidence=0.7, data_name =\"Training\", data_setting = \"Long\"):\n",
    "    \n",
    "    prediction_df = \"/home/jik19004/FilesToRun/BayesianTimeSeries/{}{}/{}Predictions{}_Gaussian.csv\".format(data_name, data_setting, data_name, data_setting)\n",
    "    actual_df = \"/home/jik19004/FilesToRun/BayesianTimeSeries/{}{}/{}Actual_{}.csv\".format(data_name,data_setting,data_name,data_setting)\n",
    "    std_dev_df = \"/home/jik19004/FilesToRun/BayesianTimeSeries/{}{}/{}Std{}_Gaussian.csv\".format(data_name, data_setting, data_name, data_setting)\n",
    "    \n",
    "    prediction_df = pd.read_csv(prediction_df)\n",
    "    actual_df = pd.read_csv(actual_df)\n",
    "    std_dev_df = pd.read_csv(std_dev_df)\n",
    "    \n",
    "    # Calculate Z-score for the given confidence level\n",
    "    Z = norm.ppf((1 + confidence) / 2)\n",
    "\n",
    "    # Ensure numeric conversion to handle non-numeric data gracefully\n",
    "    actual_df = actual_df.apply(pd.to_numeric, errors='coerce')\n",
    "    prediction_df = prediction_df.apply(pd.to_numeric, errors='coerce')\n",
    "    std_dev_df = std_dev_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Compute the confidence interval bounds\n",
    "    lower_bounds = prediction_df - Z * std_dev_df\n",
    "    upper_bounds = prediction_df + Z * std_dev_df\n",
    "\n",
    "    # Check if actual values fall within the bounds\n",
    "    encapsulation = (actual_df >= lower_bounds) & (actual_df <= upper_bounds)\n",
    "\n",
    "    # Calculate the percentage of encapsulated values\n",
    "    encapsulation_percentage = encapsulation.sum().sum() / encapsulation.size * 100\n",
    "\n",
    "    return encapsulation_percentage\n",
    "\n",
    "# Test the function with the loaded data\n",
    "#encapsulation_result = calculate_encapsulation_percentage(data_setting = \"Short\", data_name = \"Training\", confidence = 0.95)\n",
    "#encapsulation_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval \n",
    "\n",
    "val_100_short = pd.read_csv(\"/home/jik19004/FilesToRun/BayesianTimeSeries/ValidationShort/hundred_preds_short_Gaussian.csv\")\n",
    "array = np.ones(shape = val_100_short.shape)\n",
    "\n",
    "for i in range(len(val_100_short)):\n",
    "    for j in range(val_100_short.shape[1]):\n",
    "         arr = val_100_short.iloc[i][j] # get the lsit \n",
    "         arr = literal_eval(arr) \n",
    "         arr = [float(x) for x in arr] # convert each to floats. \n",
    "         \n",
    "         array[i][j] = np.median(arr) # get the median \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking account of Laplacian Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nencapsulation_result = calculate_encapsulation_percentage(data_setting = \"Long\", data_name = \"Testing\", confidence = 0.95)\\nencapsulation_result '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm, laplace \n",
    "\n",
    "\n",
    "# Define a function to calculate the encapsulation percentage\n",
    "def calculate_encapsulation_percentage(confidence=0.7, data_name =\"Training\", data_setting = \"Long\"):\n",
    "    \n",
    "    actual_df = \"/home/jik19004/FilesToRun/BayesianTimeSeries/{}{}/{}Actual_{}.csv\".format(data_name,data_setting,data_name,data_setting)\n",
    "    predictions_100_df = \"/home/jik19004/FilesToRun/BayesianTimeSeries/{}{}/hundred_preds_{}_Gaussian.csv\".format(data_name, data_setting, data_setting)\n",
    "    predictions_100_df = pd.read_csv(predictions_100_df)\n",
    "    actual_df = pd.read_csv(actual_df)\n",
    "    \n",
    "    if \"Unnamed: 0\" in predictions_100_df.columns.values:\n",
    "        predictions_100_df.drop([\"Unnamed: 0\"], axis = 1, inplace = True)\n",
    "\n",
    "    \n",
    "    \n",
    "    median_array = np.ones(shape = predictions_100_df.shape)\n",
    "    mad_array = np.ones(shape = predictions_100_df.shape) # create for mad_array \n",
    "    \n",
    "    confidence = (1 - confidence)\n",
    "    \n",
    "    for i in range(len(predictions_100_df)):\n",
    "        for j in range(predictions_100_df.shape[1]):\n",
    "            arr = predictions_100_df.iloc[i][j] # get the lsit \n",
    "            arr = literal_eval(arr) \n",
    "            arr = [float(x) for x in arr] # convert each to floats. \n",
    "            \n",
    "            median_array[i][j] = np.median(arr) # get the median \n",
    "            \n",
    "    encapsulation_raw = 0  \n",
    "    exceedance_lower = 0 \n",
    "    exceedance_upper = 0            \n",
    "    for i in range(len(predictions_100_df)):\n",
    "        for j in range(predictions_100_df.shape[1]):\n",
    "            arr = predictions_100_df.iloc[i][j] # get the lsit \n",
    "            arr = literal_eval(arr) \n",
    "            arr = [float(x) for x in arr] # convert each to floats. \n",
    "            \n",
    "            mad_array[i][j] = np.median(np.abs(arr - median_array[i][j])) # get the mad value.  \n",
    "            b = mad_array[i][j] / np.log(2)\n",
    "            \n",
    "            ci_lower = median_array[i][j] + b * np.log(confidence)\n",
    "            ci_upper = median_array[i][j] - b * np.log(confidence)\n",
    "            \n",
    "            if ci_lower <= actual_df.iloc[i][j] and ci_upper >= actual_df.iloc[i][j]:\n",
    "                encapsulation_raw +=1 \n",
    "            elif actual_df.iloc[i][j] < ci_lower:\n",
    "                exceedance_lower += 1\n",
    "            else:\n",
    "                exceedance_upper += 1 \n",
    "            \n",
    "        \n",
    "\n",
    "    # Calculate the percentage of encapsulated values\n",
    "    encapsulation_percentage = encapsulation_raw / (predictions_100_df.shape[0] * predictions_100_df.shape[1]) * 100\n",
    "    exceedance_upper = exceedance_upper/(predictions_100_df.shape[0] * predictions_100_df.shape[1]) * 100\n",
    "    exceedance_lower = exceedance_lower/(predictions_100_df.shape[0] * predictions_100_df.shape[1]) * 100\n",
    "\n",
    "    return encapsulation_percentage, exceedance_upper, exceedance_lower\n",
    "\n",
    "# Test the function with the loaded data\n",
    "\"\"\"\n",
    "encapsulation_result = calculate_encapsulation_percentage(data_setting = \"Long\", data_name = \"Testing\", confidence = 0.95)\n",
    "encapsulation_result \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceedance Frequency Both Upper and Lower "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:22: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:22: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/tmp/ipykernel_30616/1853143965.py:22: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if bound_setting is \"Upper\":\n"
     ]
    }
   ],
   "source": [
    "def calculate_encapsulation_percentage(confidence = 0.7, data_name =\"Training\", data_setting = \"Long\", bound_setting = \"Upper\"):\n",
    "    prediction_df = \"/home/jik19004/FilesToRun/BayesianTimeSeries/{}{}/{}Predictions{}_Gaussian.csv\".format(data_name, data_setting, data_name, data_setting) \n",
    "    actual_df = \"/home/jik19004/FilesToRun/BayesianTimeSeries/{}{}/{}Actual_{}.csv\".format(data_name,data_setting,data_name,data_setting) \n",
    "    std_dev_df = \"/home/jik19004/FilesToRun/BayesianTimeSeries/{}{}/{}Std{}_Gaussian.csv\".format(data_name, data_setting, data_name, data_setting) \n",
    "    \n",
    "    prediction_df = pd.read_csv(prediction_df) \n",
    "    actual_df = pd.read_csv(actual_df) \n",
    "    std_dev_df = pd.read_csv(std_dev_df)  \n",
    "\n",
    "    Z = norm.ppf((1 + confidence) / 2) \n",
    "\n",
    "\n",
    "    actual_df = actual_df.apply(pd.to_numeric, errors='coerce') \n",
    "    prediction_df = prediction_df.apply(pd.to_numeric, errors='coerce') \n",
    "    std_dev_df = std_dev_df.apply(pd.to_numeric, errors='coerce') \n",
    "\n",
    "\n",
    "    lower_bounds = prediction_df - Z * std_dev_df \n",
    "    upper_bounds = prediction_df + Z * std_dev_df \n",
    "\n",
    "    excedence = 0\n",
    "    if bound_setting is \"Upper\":\n",
    "        excedence = actual_df > upper_bounds # strictly greater than upper bound. \n",
    "    else:\n",
    "        excedence = actual_df < lower_bounds \n",
    "    \n",
    "    encapsulation_percentage = excedence.sum().sum() / excedence.size * 100\n",
    "\n",
    "    return encapsulation_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.03741886216113\n",
      "1.9186712485681559\n",
      "90.04390988927071\n"
     ]
    }
   ],
   "source": [
    "value_upper = calculate_encapsulation_percentage(confidence=0.7, data_name=\"Testing\", data_setting = \"Short\", bound_setting=\"Upper\")\n",
    "value_lower = calculate_encapsulation_percentage(confidence=0.7, data_name=\"Testing\", data_setting = \"Short\", bound_setting=\"Lower\")\n",
    "print(value_upper)\n",
    "print(value_lower)\n",
    "print(100-(value_lower + value_upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'no' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_285291/3395482557.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'no' is not defined"
     ]
    }
   ],
   "source": [
    "no"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
